### Phase 5 â€” Failure Testing (Proof of Reliability)

**Objective**

Demonstrate that the system is resilient, self-healing, and observable under real failure scenarios. Phase 5 validates that core AWS services (ECS, ALB, Auto Scaling, and RDS) respond correctly to infrastructure stress, recover automatically, and return the system to a stable state without manual intervention.

---

### Test 1 â€” Kill Container (ECS Self-Healing)

**Purpose**

Prove that if a running container fails unexpectedly, Amazon ECS automatically replaces the task and restores the service to a steady state.

**Test Procedure**

- Navigated to ECS â†’ Cluster â†’ Service â†’ Tasks

- Selected one running task

- Manually stopped the task to simulate a container crash

**Observations**

- The stopped task immediately transitioned to STOPPED

- ECS detected the failure and launched a replacement task

- The service returned to a stable state with the desired number of tasks running

- No user-facing downtime occurred

**Evidence**

ECS task list before termination

Task transitioning to STOPPING/STOPPED

New task launching and service stabilizing

task event logs

ðŸ“¸ evidence/screenshots/phase-5/test-1-kill-container/

**Outcome**

âœ… PASS â€” ECS successfully demonstrated self-healing behavior.

---

### Test 2 â€” Load Spike (Service Auto Scaling)

**Purpose**

Validate that increased traffic causes the ECS service to scale out automatically and return to baseline once load subsides.

**Precondition**

ECS Service Auto Scaling enabled.

Scaling policy based on request load.

Minimum and maximum task counts configured.

**Test Procedure**

Verified auto scaling policy under ECS Service â†’ Auto Scaling

Generated load using ApacheBench:
ab -n 5000 -c 400 http://<ALB-DNS>/health

Monitored CloudWatch dashboard during the test

**Observations**

ALB RequestCount spiked during the load test

ECS DesiredTaskCount increased in response

ECS launched additional tasks to handle traffic

After traffic ended, tasks scaled back down to baseline

Response times remained stable

**Evidence**

Auto scaling policy configuration

CloudWatch graphs showing:

RequestCount spike

DesiredTaskCount increase

Return to baseline after cooldown

ðŸ“¸ evidence/screenshots/phase-5/test-2-load-spike/

**Outcome**

âœ… PASS â€” Auto scaling reacted correctly and preserved performance.

---

### Test 3A â€” RDS Restart (Transient Database Outage)

**Purpose**

Validate application behavior during a short-lived database interruption, simulating a transient failure such as a database reboot or maintenance event.

This test confirms that:
The application remains running during a database restart

    Database errors surface correctly

    The application automatically reconnects once the database becomes available

**Procedure**

- Step A â€” Baseline
  Continuously queried the /db-check endpoint to confirm normal operation

Verified database connectivity prior to restart:
while true; do
curl -s http://<ALB-DNS>/db-check
echo
sleep 2
done

ðŸ“¸ before-db-restart-db-check.png

- Step B â€” Restart RDS

Restarted the RDS instance via RDS Console â†’ Actions â†’ Reboot

This introduced a short database unavailability window

ðŸ“¸ db-reboot.png

- Step C â€” Observe Failure

/db-check returned database connection errors during the restart window

Errors were surfaced without impacting application health

ALB health checks remained green

ECS tasks were not replaced

ðŸ“¸ during-db-restart-failure.png

- Step D â€” Confirm Recovery

/db-check automatically returned success once RDS was available

No application restart or manual intervention required

Database connections were re-established dynamically

ðŸ“¸ after-db-restart-success.png

**Result**

âœ… PASS â€” The system handled a transient database outage gracefully and recovered automatically.

---

### Test 3B â€” Stop & Start RDS (Intentional Full Database Outage)

**Purpose**

Demonstrate application behavior during a full database outage and confirm recovery once the database is restored.

Due to Free Tier limitations, Multi-AZ failover was unavailable. This test validates controlled outage resilience instead.

**Step C1 â€” Baseline**

Verified /db-check endpoint returned a successful response

Confirmed database connectivity prior to outage

ðŸ“¸ before-db-stop-db-check.png

**Step C2 â€” Stop RDS Instance**

Navigated to RDS Console â†’ Actions â†’ Stop

This simulated a planned database outage

**Step C3 â€” Observe Failure**

Repeated /db-check requests while RDS was stopped

Endpoint returned connection errors as expected

Application remained running and responsive

ðŸ“¸ during-db-stop-failure.png
ðŸ“¸ db-stop-inprogress.png
ðŸ“¸ db-stop-inprogress-graphs.png
ðŸ“¸ cloudwatch-response-drop.png
ðŸ“¸ alb-healthy-response.png

**Step C4 â€” Start RDS Instance**

Restarted the RDS instance

Waited for database to become available (5â€“10 minutes)

**Step C5 â€” Confirm Recovery**

/db-check returned success again

Database connections were restored

No ECS task replacement or cascading failures occurred

ðŸ“¸ after-db-start-success.png
ðŸ“¸cloudwatch-response-spike.png

**Key Design Validation**

/health endpoint does not depend on the database

ECS and ALB did not terminate tasks during DB outage

Application degraded gracefully and recovered automatically

**Outcome**

âœ… PASS â€” System handled full DB outage and recovery correctly.

---

**Final Conclusion**

Phase 5 confirms that the system behaves as a production-ready, fault-tolerant architecture:

Failures are detected automatically

Recovery occurs without manual intervention

Scaling responds to real traffic demand

Dependencies fail gracefully without cascading impact

This phase demonstrates practical application of AWS reliability principles, including self-healing infrastructure, observability, and graceful degradation.
